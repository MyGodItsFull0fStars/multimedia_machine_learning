{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_all, y_train_all), (X_test_all, y_test_all) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results: bool = False\n",
    "svm_kernel = ['linear', 'poly', 'sigmoid']\n",
    "svm_gamma = [0.1, 0.5, 1.0, 'scale']\n",
    "svm_class_weights = 'balanced'\n",
    "pca_num_components: int = [1, 3, 5, 10, 15]\n",
    "forest_depth = [5, 10, None]\n",
    "forest_number_of_estimators = [1, 25, 50, 100]\n",
    "forest_criterion = ['gini', 'entropy', 'log_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_train_all\n",
    "\n",
    "for i in range(1, 7):\n",
    "    subplot_idx = 430 + i\n",
    "    plt.subplot(subplot_idx)\n",
    "    plt.imshow((images[i]), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute the indices randomly with `np.random.permutation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_permutation = np.random.permutation(len(X_train_all))\n",
    "index_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = X_train_all[index_permutation]\n",
    "y_train_all = y_train_all[index_permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset index\n",
    "\n",
    "Resets the indices of the rows to be ascending again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes: list = [x for x in range(1000, 20001, 1000)]\n",
    "test_sizes: list = [x // 2 for x in train_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = X_train_all / 255\n",
    "X_test_all = X_test_all / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def include_pca_preprocessing(data: np.ndarray, num_components: int) -> pd.DataFrame:\n",
    "    standard_data = StandardScaler().fit_transform(data)\n",
    "    pca = PCA(num_components)\n",
    "    X_pca = pca.fit_transform(standard_data)\n",
    "    return X_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time_fit(estimator, X, y, time_df: pd.DataFrame = None):\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    estimator.fit(X,y)\n",
    "    compute_time = time.perf_counter() - start_time\n",
    "    print(compute_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_time(estimator, X_train, y_train, X_test, y_test, train_size: int, mode: str):\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    estimator.fit(X_train, y_train)\n",
    "    compute_time_train = time.perf_counter() - start_time\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    y_hat = estimator.predict(X_test)\n",
    "    compute_time_predict = time.perf_counter() - start_time\n",
    "    \n",
    "    if save_results:\n",
    "        \n",
    "        report = classification_report(y_test, y_hat, output_dict=True, zero_division=False)\n",
    "        df_svm = pd.DataFrame(report).transpose()\n",
    "        df_svm[['train time', 'predict time']] = compute_time_train, compute_time_predict\n",
    "        \n",
    "        save_path_csv: str = f'./results/csv/{train_size}_{mode}' \n",
    "        save_path_png: str = f'./results/figures/{train_size}_{mode}' \n",
    "        \n",
    "        df_svm.to_csv(f'{save_path_csv}.csv')\n",
    "        \n",
    "        confusion_matrix = metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_hat)\n",
    "        conf_mode = mode.replace('_', ' ').capitalize()\n",
    "        confusion_matrix.figure_.suptitle(f'Confusion Matrix - {conf_mode} Trainsize {train_size}')\n",
    "        plt.savefig(f'{save_path_png}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_sizes)):\n",
    "\n",
    "    train_size = train_sizes[idx]\n",
    "    test_size = test_sizes[idx]\n",
    "    X_train, X_test = X_train_all[:train_size], X_test_all[:test_size]\n",
    "    y_train, y_test = y_train_all[:train_size], y_test_all[:test_size]\n",
    "\n",
    "    X_train = X_train.reshape(len(X_train), -1)\n",
    "    X_test = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "    for gam in svm_gamma:\n",
    "        svm = SVC(gamma=gam)\n",
    "\n",
    "        log_time(svm, X_train, y_train, X_test, y_test,\n",
    "                 train_size, f'svm_gamma_{gam}')\n",
    "\n",
    "    for kern in svm_kernel:\n",
    "        svm = SVC(kernel=kern)\n",
    "\n",
    "        log_time(svm, X_train, y_train, X_test, y_test,\n",
    "                 train_size, f'svm_kernel_{kern}')\n",
    "\n",
    "    for C in range(1, 5):\n",
    "        svm = SVC(C=C)\n",
    "        log_time(svm, X_train, y_train, X_test,\n",
    "                 y_test, train_size, f'svm_C_{C}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_sizes)):\n",
    "    \n",
    "    train_size = train_sizes[idx]\n",
    "    test_size = test_sizes[idx]  \n",
    "\n",
    "    for pca_num in pca_num_components:\n",
    "        X_train, X_test = X_train_all[:train_size], X_test_all[:test_size]\n",
    "        y_train, y_test = y_train_all[:train_size], y_test_all[:test_size]\n",
    "        \n",
    "        X_train = X_train.reshape(len(X_train), -1)\n",
    "        X_test = X_test.reshape(len(X_test), -1)\n",
    "        \n",
    "        X_train = include_pca_preprocessing(X_train, num_components=pca_num)\n",
    "        X_test = include_pca_preprocessing(X_test, num_components=pca_num)\n",
    "        \n",
    "\n",
    "        for kern in svm_kernel:\n",
    "            svm = SVC(kernel=kern)\n",
    "\n",
    "            log_time(svm, X_train, y_train, X_test, y_test,\n",
    "                    train_size, f'pca_{pca_num}_svm_kernel_{kern}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_sizes)):\n",
    "\n",
    "    train_size = train_sizes[idx]\n",
    "    test_size = test_sizes[idx]\n",
    "    X_train, X_test = X_train_all[:train_size], X_test_all[:test_size]\n",
    "    y_train, y_test = y_train_all[:train_size], y_test_all[:test_size]\n",
    "\n",
    "    X_train = X_train.reshape(len(X_train), -1)\n",
    "    X_test = X_test.reshape(len(X_test), -1)\n",
    "    \n",
    "    for depth in forest_depth:\n",
    "        random_forest = RandomForestClassifier(max_depth=depth)\n",
    "        \n",
    "        file_label: str = f'forest_depth_' \n",
    "        file_label += str(depth) if depth is not None else 'Any'\n",
    "        \n",
    "        log_time(random_forest, X_train, y_train, X_test, y_test,\n",
    "                 train_size, file_label)\n",
    "        \n",
    "    for num_est in forest_number_of_estimators:\n",
    "        random_forest = RandomForestClassifier(n_estimators=num_est)\n",
    "        \n",
    "        file_label: str = f'forest_num_estimators_{num_est}'\n",
    "        \n",
    "        log_time(random_forest, X_train, y_train, X_test, y_test,\n",
    "                 train_size, file_label)\n",
    "        \n",
    "    for criterion in forest_criterion:\n",
    "        random_forest = RandomForestClassifier(criterion=criterion)\n",
    "        \n",
    "        file_label: str = f'forest_criterion_{criterion}'\n",
    "        \n",
    "        log_time(random_forest, X_train, y_train, X_test, y_test,\n",
    "                 train_size, file_label)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sklearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a5bd0160c479b3e51ed66060515b262fb0f0f37133b4cd3b90cf27d5f641f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
